import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import re
import json
import os


def clean_price(price_text):
    # Fiyat metninden sadece sayÄ±larÄ± alÄ±r
    return re.sub(r'[^\d]', '', price_text)


def save_links_to_file(links, filename="buca_links.json"):
    #Linkleri JSON dosyasÄ±na kaydet
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)
    print(f"âœ“ {len(links)} link '{filename}' dosyasÄ±na kaydedildi.")


def load_links_from_file(filename="buca_links.json"):
    #JSON dosyasÄ±ndan linkleri yÃ¼kle
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            links = json.load(f)
        print(f"âœ“ {len(links)} link '{filename}' dosyasÄ±ndan yÃ¼klendi.")
        return set(links)
    return set()


def save_progress(data, filename="buca_progress.json"):
    # Ä°lerlemeyi kaydet (hangi linkler iÅŸlendi)
    progress = {
        'processed_links': [d['Link'] for d in data],
        'last_updated': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_processed': len(data)
    }
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def load_progress(filename="buca_progress.json"):
    # KaydedilmiÅŸ ilerlemeyi yÃ¼kle
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            progress = json.load(f)
        print(f"âœ“ Ä°lerleme yÃ¼klendi: {progress['total_processed']} ilan iÅŸlenmiÅŸ.")
        return set(progress['processed_links'])
    return set()


def scroll_page_smoothly(driver, pause_time=1.0):
    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_attempts = 0
    max_attempts = 8

    while scroll_attempts < max_attempts:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause_time)
        new_height = driver.execute_script("return document.body.scrollHeight")

        if new_height == last_height:
            scroll_attempts += 1
            if scroll_attempts >= 2:
                break
        else:
            scroll_attempts = 0
            last_height = new_height

    driver.execute_script("window.scrollTo(0, 0);")
    time.sleep(0.3)


def get_all_listing_links(driver, start_page=1):
    base_url = "https://www.hepsiemlak.com/en/buca-satilik?page="

    # Ã–nceden kaydedilmiÅŸ linkleri yÃ¼kle
    all_links = load_links_from_file()
    initial_count = len(all_links)

    current_page = start_page
    consecutive_empty_pages = 0
    max_empty_pages = 2

    print(f"\nAÅŸama 1: Buca SatÄ±lÄ±k ilan linkleri toplanÄ±yor...")
    if initial_count > 0:
        print(f"Daha Ã¶nce {initial_count} link toplanmÄ±ÅŸ. Sayfa {start_page}'den devam ediliyor...")

    cookie_handled = (start_page > 1)  # EÄŸer sayfa 1'den baÅŸlamÄ±yorsak cookie zaten halledilmiÅŸ

    try:
        while True:
            if current_page == 1:
                url = "https://www.hepsiemlak.com/en/buca-satilik"
            else:
                url = base_url + str(current_page)

            print(f"\n{'=' * 50}")
            print(f"Sayfa {current_page} taranÄ±yor... (Toplam link: {len(all_links)})")
            print(f"{'=' * 50}")

            try:
                driver.get(url)
            except Exception as e:
                print(f"âœ— Sayfa yÃ¼klenirken hata: {e}")
                break

            if current_page == 1 and not cookie_handled:
                print("\n" + "=" * 50)
                print("TARAYICI AÃ‡ILDI. SCRIPT DURDURULDU.")
                print("LÃ¼tfen aÃ§Ä±lan Edge penceresine gidin ve Ã‡EREZ (COOKIE) uyarÄ±sÄ±nÄ±")
                print("manuel olarak ('Accept All' vb.) tÄ±klayarak kapatÄ±n.")
                print("\nÄ°ÅŸlemi bitirince bu terminal ekranÄ±na dÃ¶nÃ¼n ve")
                print("DEVAM ETMEK Ä°Ã‡Ä°N ENTER TUÅUNA BASIN...")
                print("=" * 50)
                input()
                print("KullanÄ±cÄ± onayÄ± alÄ±ndÄ±, script devam ediyor...\n")
                cookie_handled = True
                time.sleep(2)

            time.sleep(1)

            # Lazy loading iÃ§in sayfayÄ± kaydÄ±r
            scroll_page_smoothly(driver, pause_time=1.0)

            link_selectors = [
                "a.card-link",
                "li.list-view-item a.card-link",
                "li.list-view-item a",
            ]

            listings = []
            for selector in link_selectors:
                temp_listings = driver.find_elements(By.CSS_SELECTOR, selector)
                if temp_listings and len(temp_listings) > len(listings):
                    listings = temp_listings

            if not listings:
                consecutive_empty_pages += 1
                print(f"âš ï¸ Bu sayfada ilan bulunamadÄ±. BoÅŸ sayfa: {consecutive_empty_pages}/{max_empty_pages}")

                if consecutive_empty_pages >= max_empty_pages:
                    print(f"\n{max_empty_pages} ardÄ±ÅŸÄ±k boÅŸ sayfa. Ä°ÅŸlem sonlandÄ±rÄ±lÄ±yor.")
                    break

                time.sleep(2)
                current_page += 1
                continue
            else:
                consecutive_empty_pages = 0

            # Linkleri topla
            links_before = len(all_links)

            for listing in listings:
                try:
                    link = listing.get_attribute("href")
                    if link and link.startswith("http") and "satilik" in link:
                        all_links.add(link)
                except Exception as e:
                    pass

            new_links = len(all_links) - links_before
            print(f"âœ“ Bu sayfadan {new_links} yeni link eklendi")
            print(f"Toplam benzersiz link: {len(all_links)}")

            # Her 5 sayfada bir kaydet
            if current_page % 5 == 0:
                save_links_to_file(all_links)
                print(f"ğŸ’¾ Ä°lerleme kaydedildi (Sayfa {current_page})")

            if new_links == 0:
                print("âš ï¸ Yeni link eklenemedi. Son sayfaya ulaÅŸÄ±ldÄ± olabilir.")
                break

            current_page += 1
            time.sleep(1.5)  # Rate limiting

    except KeyboardInterrupt:
        print("\n\nâš ï¸ KullanÄ±cÄ± tarafÄ±ndan durduruldu!")
        save_links_to_file(all_links)
        print(f"Son durum: Sayfa {current_page}, Toplam {len(all_links)} link")
        raise
    except Exception as e:
        print(f"\nâœ— Beklenmeyen hata: {e}")
        save_links_to_file(all_links)
        raise

    # Son kayÄ±t
    save_links_to_file(all_links)
    return list(all_links)


def scrape_detail_page(driver, link):
    """Tek bir ilan detay sayfasÄ±nÄ± ziyaret eder ve tÃ¼m Ã¶zellikleri Ã§eker."""
    try:
        driver.get(link)
    except Exception as e:
        print(f"âœ— Sayfa aÃ§Ä±lamadÄ±: {e}")
        return None

    data = {"Link": link}

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "h1.fontRB"))
        )

        try:
            title_elem = driver.find_element(By.CSS_SELECTOR, "h1.fontRB")
            data['Baslik'] = title_elem.text.strip()
        except NoSuchElementException:
            data['Baslik'] = "N/A"

        try:
            price_elem = driver.find_element(By.CSS_SELECTOR, "p.fz24-text.price")
            price_text = price_elem.text.strip()
            data['Fiyat (Metin)'] = price_text
            data['Fiyat (Sayisal)'] = re.sub(r'[^\d]', '', price_text)
        except NoSuchElementException:
            data['Fiyat (Metin)'] = "N/A"
            data['Fiyat (Sayisal)'] = "N/A"

        try:
            location_elements = driver.find_elements(By.CSS_SELECTOR, "div.bread-crumb ul li a span")
            location_text = " > ".join([elem.text for elem in location_elements if elem.text])
            data['Konum'] = location_text if location_text else "N/A"
        except NoSuchElementException:
            data['Konum'] = "N/A"

        feature_items = driver.find_elements(By.CSS_SELECTOR, "ul.adv-info-list li.spec-item")
        for item in feature_items:
            try:
                key = item.find_element(By.CSS_SELECTOR, "span.txt").text.strip()
                all_text = item.text
                value = all_text.replace(key, '').strip()
                key = key.replace('/', ' / ')
                if key:
                    data[key] = value if value else "N/A"
            except Exception:
                continue

        return data

    except TimeoutException:
        print(f"âš ï¸ Sayfa zaman aÅŸÄ±mÄ±na uÄŸradÄ±: {link}")
        return None
    except Exception as e:
        print(f"âš ï¸ Beklenmeyen hata: {e}")
        return None


def scrape_details_from_links(driver, links, output_file="buca_emlak_detayli_SATILIK_veriler.csv"):

    # Daha Ã¶nce iÅŸlenmiÅŸ linkleri yÃ¼kle
    processed_links = load_progress()

    # Daha Ã¶nce kaydedilmiÅŸ veriyi yÃ¼kle
    all_listings_data = []
    if os.path.exists(output_file):
        try:
            df_existing = pd.read_csv(output_file, encoding='utf-8-sig')
            all_listings_data = df_existing.to_dict('records')
            print(f"âœ“ Ã–nceki veriler yÃ¼klendi: {len(all_listings_data)} ilan")
        except:
            pass

    # HenÃ¼z iÅŸlenmemiÅŸ linkleri filtrele
    remaining_links = [link for link in links if link not in processed_links]

    print(f"\n{'=' * 50}")
    print(f"AÅŸama 2: Detay Bilgileri Ã‡ekiliyor")
    print(f"Toplam link: {len(links)}")
    print(f"Ä°ÅŸlenmiÅŸ: {len(processed_links)}")
    print(f"Kalan: {len(remaining_links)}")
    print(f"{'=' * 50}\n")

    if not remaining_links:
        print("âœ“ TÃ¼m ilanlar zaten iÅŸlenmiÅŸ!")
        return all_listings_data

    try:
        for i, link in enumerate(remaining_links):
            print(f"\n--- Ä°lan {i + 1} / {len(remaining_links)} (Toplam: {len(all_listings_data) + i + 1}) ---")
            print(f"Link: {link[:60]}...")

            details = scrape_detail_page(driver, link)
            if details:
                all_listings_data.append(details)
                print(f"âœ“ BaÅŸarÄ±lÄ±: {details.get('Baslik', 'N/A')[:50]}...")
            else:
                print(f"âœ— Detay Ã§ekilemedi")

            # Her 10 ilanda bir kaydet
            if (i + 1) % 10 == 0:
                df = pd.DataFrame(all_listings_data)
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                save_progress(all_listings_data)
                print(f"ğŸ’¾ Ä°lerleme kaydedildi ({len(all_listings_data)} ilan)")

            time.sleep(1.0)  # Rate limiting

    except KeyboardInterrupt:
        print("\n\nâš ï¸ KullanÄ±cÄ± tarafÄ±ndan durduruldu!")
        print("Mevcut veriler kaydediliyor...")
    except Exception as e:
        print(f"\nâœ— Hata: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Her durumda kaydet
        if all_listings_data:
            df = pd.DataFrame(all_listings_data)
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            save_progress(all_listings_data)
            print(f"ğŸ’¾ Veriler kaydedildi: {len(all_listings_data)} ilan")

    return all_listings_data


def run_scraper(mode="full", start_page=1):
    """
    Ana scraper fonksiyonu
    """

    options = webdriver.EdgeOptions()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option('excludeSwitches', ['enable-automation'])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
    )

    driver_path = "msedgedriver.exe"
    service = Service(executable_path=driver_path)

    try:
        driver = webdriver.Edge(service=service, options=options)
        print("âœ“ Edge WebDriver baÅŸarÄ±yla baÅŸlatÄ±ldÄ±.")
    except Exception as e:
        print(f"âœ— HATA: 'msedgedriver.exe' baÅŸlatÄ±lamadÄ±. Hata: {e}")
        return

    all_listings_data = []

    try:
        # Link toplama aÅŸamasÄ±
        if mode in ["full", "links"]:
            all_links = get_all_listing_links(driver, start_page=start_page)
        else:
            # Sadece detay Ã§ekme modunda Ã¶nceki linkleri yÃ¼kle
            all_links = list(load_links_from_file())
            if not all_links:
                print("âœ— HiÃ§ link bulunamadÄ±! Ã–nce 'links' modunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                return

        if not all_links:
            print("\nâš ï¸ HiÃ§ ilan linki bulunamadÄ±.")
            return

        # Detay Ã§ekme aÅŸamasÄ±
        if mode in ["full", "details"]:
            all_listings_data = scrape_details_from_links(driver, all_links)

    finally:
        driver.quit()

        if all_listings_data:
            print(f"\n{'=' * 50}")
            print(f"âœ“ Ä°ÅLEM TAMAMLANDI!")
            print(f"Toplam {len(all_listings_data)} ilan iÅŸlendi")
            print(f"Dosya: buca_emlak_detayli_SATILIK_veriler.csv")
            print(f"{'=' * 50}")


if __name__ == "__main__":
    # 1. STANDART
    # run_scraper(mode="full", start_page=1)

    # 2. SADECE LÄ°NK TOPLA
    # run_scraper(mode="links", start_page=1)

    # 3. KALDIÄI YERDEN DEVAM ET
    # run_scraper(mode="links", start_page=41)

    # 4. SADECE DETAY Ã‡EK

     run_scraper(mode="details")